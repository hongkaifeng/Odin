import subprocess
import os
import sys
from typing import List


def run_commands(commands: List[str], use_shell: bool = True, encoding: str = "utf-8") -> None:
    """
    åœ¨ç»ˆç«¯è¿ç»­æ‰§è¡Œä¸€ç³»åˆ—å‘½ä»¤

    Args:
        commands: å‘½ä»¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€æ¡ç»ˆç«¯å‘½ä»¤ï¼ˆå­—ç¬¦ä¸²ï¼‰
        use_shell: æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿ Shell æ‰§è¡Œå‘½ä»¤ï¼ˆTrue æ”¯æŒç®¡é“ã€é€šé…ç¬¦ç­‰é«˜çº§è¯­æ³•ï¼‰
        encoding: å‘½ä»¤è¾“å‡ºçš„ç¼–ç æ ¼å¼ï¼ˆé»˜è®¤ utf-8ï¼ŒWindows å¯å°è¯• "gbk"ï¼‰
    """
    # è®°å½•å½“å‰å·¥ä½œç›®å½•ï¼ˆç”¨äºå¤„ç† cd å‘½ä»¤ï¼Œé¿å…å­è¿›ç¨‹ç›®å½•ä¸ç»§æ‰¿é—®é¢˜ï¼‰
    current_dir = os.getcwd()

    for idx, cmd in enumerate(commands, 1):
        cmd = cmd.strip()  # å»é™¤å‘½ä»¤å‰åç©ºæ ¼
        if not cmd:  # è·³è¿‡ç©ºå‘½ä»¤
            continue

        print(f"\n=== å¼€å§‹æ‰§è¡Œç¬¬ {idx} æ¡å‘½ä»¤ï¼š{cmd} ===")
        try:
            # ç‰¹æ®Šå¤„ç† cd å‘½ä»¤ï¼ˆsubprocess æ‰§è¡Œ cd ä»…å½±å“å­è¿›ç¨‹ï¼Œéœ€æ‰‹åŠ¨æ›´æ–°å½“å‰ç›®å½•ï¼‰
            if cmd.lower().startswith("cd "):
                # æå–ç›®æ ‡ç›®å½•ï¼ˆå¤„ç† "cd ./test" æˆ– "cd /home/user" ç­‰æ ¼å¼ï¼‰
                target_dir = cmd.split("cd ", 1)[1].strip()
                # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼ˆåŸºäºå½“å‰å·¥ä½œç›®å½•ï¼‰
                target_dir = os.path.abspath(os.path.join(current_dir, target_dir))

                # éªŒè¯ç›®å½•æ˜¯å¦å­˜åœ¨
                if not os.path.exists(target_dir):
                    raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨ï¼š{target_dir}")
                if not os.path.isdir(target_dir):
                    raise NotADirectoryError(f"ä¸æ˜¯ç›®å½•ï¼š{target_dir}")

                # æ›´æ–°å½“å‰å·¥ä½œç›®å½•
                os.chdir(target_dir)
                current_dir = target_dir
                print(f"âœ… ç›®å½•åˆ‡æ¢æˆåŠŸï¼Œå½“å‰ç›®å½•ï¼š{current_dir}")
                continue

            # æ‰§è¡Œæ™®é€šå‘½ä»¤ï¼ˆé cdï¼‰
            result = subprocess.run(
                cmd,
                shell=use_shell,
                cwd=current_dir,  # åŸºäºå½“å‰å·¥ä½œç›®å½•æ‰§è¡Œå‘½ä»¤
                stdout=subprocess.PIPE,  # æ•è·æ ‡å‡†è¾“å‡º
                stderr=subprocess.PIPE,  # æ•è·æ ‡å‡†é”™è¯¯
                encoding=encoding,
                timeout=None  # å‘½ä»¤è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´
            )

            # æ‰“å°å‘½ä»¤è¾“å‡ºï¼ˆæ ‡å‡†è¾“å‡º + æ ‡å‡†é”™è¯¯ï¼‰
            if result.stdout:
                print(f"ğŸ“¤ å‘½ä»¤è¾“å‡ºï¼š\n{result.stdout}")
            if result.stderr:
                print(f"âš ï¸  å‘½ä»¤è­¦å‘Š/é”™è¯¯ï¼š\n{result.stderr}")

            # æ£€æŸ¥å‘½ä»¤æ˜¯å¦æ‰§è¡ŒæˆåŠŸï¼ˆè¿”å›ç ä¸º 0 è¡¨ç¤ºæˆåŠŸï¼‰
            result.check_returncode()
            print(f"âœ… ç¬¬ {idx} æ¡å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")

        except subprocess.TimeoutExpired:
            print(f"âŒ ç¬¬ {idx} æ¡å‘½ä»¤è¶…æ—¶ï¼ˆè¶…è¿‡ {300} ç§’ï¼‰")
            sys.exit(1)  # è¶…æ—¶å¯é€‰æ‹©é€€å‡ºæˆ–ç»§ç»­ï¼Œæ­¤å¤„é»˜è®¤é€€å‡º
        except subprocess.CalledProcessError as e:
            print(f"âŒ ç¬¬ {idx} æ¡å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼ˆè¿”å›ç ï¼š{e.returncode}ï¼‰")
            print(f"   é”™è¯¯è¯¦æƒ…ï¼š{e.stderr}")
            sys.exit(1)  # å‘½ä»¤å¤±è´¥å¯é€‰æ‹©é€€å‡ºæˆ–ç»§ç»­ï¼Œæ­¤å¤„é»˜è®¤é€€å‡º
        except Exception as e:
            print(f"âŒ ç¬¬ {idx} æ¡å‘½ä»¤å¤„ç†å¼‚å¸¸ï¼š{str(e)}")
            sys.exit(1)

    print("\nğŸ‰ æ‰€æœ‰å‘½ä»¤å‡æ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    # --------------------------
    # æ ¸å¿ƒé…ç½®ï¼šæ›¿æ¢ä¸ºä½ çš„å‘½ä»¤åˆ—è¡¨
    # --------------------------
    # ç¤ºä¾‹ 1ï¼šLinux/macOS ç¯å¢ƒï¼ˆæ›´æ–°åŒ… + å®‰è£…ä¾èµ– + æŸ¥çœ‹ç›®å½•ï¼‰
    # commands = [
    #     "sudo apt update && sudo apt upgrade -y",  # Ubuntu æ›´æ–°ç³»ç»Ÿ
    #     "pip install numpy pandas",  # å®‰è£… Python ä¾èµ–
    #     "cd ./project",  # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
    #     "ls -l",  # æŸ¥çœ‹ç›®å½•æ–‡ä»¶è¯¦æƒ…
    #     "python main.py --epochs 10"  # è¿è¡Œ Python è„šæœ¬
    # ]

    # ç¤ºä¾‹ 2ï¼šWindows ç¯å¢ƒï¼ˆæŸ¥çœ‹ç›®å½• + å®‰è£…ä¾èµ– + è¿è¡Œè„šæœ¬ï¼‰
    commands = [
        "dir",  # æŸ¥çœ‹å½“å‰ç›®å½•æ–‡ä»¶ï¼ˆWindows CMDï¼‰
        "pip install requests",  # å®‰è£… Python ä¾èµ–
        "cd ./test_data",  # åˆ‡æ¢åˆ°æ•°æ®ç›®å½•
        "dir /s",  # é€’å½’æŸ¥çœ‹ç›®å½•æ–‡ä»¶ï¼ˆWindows CMDï¼‰
        "python process_data.py"  # è¿è¡Œæ•°æ®å¤„ç†è„šæœ¬
    ]
    from pathlib import Path

    randomseed=[42]

    datasets=["obgn3000","obgn6000","obgn10000","cora","citeseer","GLcora"]
    #datasets = ["cora","citeseer"]
    #datasets = ["ARXIV2023", "obgn-produce"]
    #datasets=["obgn6000","obgn10000","ARXIV2023"]
    datasets=["cora"]

    labelnum={"obgn3000":9,"obgn6000":9,"obgn10000":9,"GLcora":7,"cora":7,"citeseer":6,"ARXIV2023":9,"obgn-produce":11}
    linkbias=[]
    classbias=[]
    lmdatasets=["obgn3000","obgn6000","obgn10000"]
    lmdatasets =["obgn3000"]
    #lmdatasets = ["obgn6000","obgn10000","ARXIV2023"]
    retribias=[]
    rerankbia=[]
    # --------------------------
    # æ‰§è¡Œå‘½ä»¤ï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ç¼–ç ï¼‰
    # --------------------------
    # Windows è‹¥å‡ºç°ä¹±ç ï¼Œå¯å°† encoding æ”¹ä¸º "gbk"
    # run_commands(commands, encoding="gbk")"allenai/scibert_scivocab_uncased"
    modelname="TYPEV"

    #modelpath="allenai/scibert_scivocab_uncased"
    datapath="D:\mymodel\8-5maindataset"
    #run_commands(commands)
    for i in datasets:
        for j in randomseed:
            if i in linkbias:
                break
            linkcom1=f"python -m OpenLP.driver.traineval  --output_dir D:/mymodel/linkp_end  --model_name_or_path D:/mymodel/formal/{i}/{modelname}  --model_type graphformer --do_train  --save_steps 160  --eval_steps 160  --logging_steps 160 --train_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --eval_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --fp16  --per_device_train_batch_size 4  --per_device_eval_batch_size 4 --learning_rate 1e-5  --max_len 32  --num_train_epochs 100  --logging_dir D:/Patton-main/logs/sports/link_prediction  --evaluation_strategy steps --remove_unused_columns False --overwrite_output_dir True --report_to tensorboard  --seed {j}"
            linktest1=f"python -m OpenLP.driver.mytest  --output_dir D:/Patton-main/data/sports/link_prediction/tmp  --model_name_or_path D:\mymodel\linkp_end  --tokenizer_name D:/mymodel/formal/{i}/{modelname} --model_type graphformer --do_eval  --train_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --eval_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --fp16  --per_device_eval_batch_size 4 --max_len 32  --evaluation_strategy steps --remove_unused_columns False --overwrite_output_dir True --dataloader_num_workers 0  --seed {j}"
            run_commands([linkcom1,linktest1], encoding="gbk")

        for j in randomseed:
            if i in classbias:
                break
            numm = labelnum[i]
            classcom1 = f"python -m OpenLP.driver.trainclasseval  --output_dir D:/mymodel/classend  --model_name_or_path D:/mymodel/formal/{i}/{modelname}  --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --do_train  --save_steps {numm * 20}  --eval_steps {numm * 20}  --logging_steps {numm * 20} --train_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --eval_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --class_num {numm}  --fp16  --per_device_train_batch_size 4  --per_device_eval_batch_size 4  --learning_rate 1e-5  --max_len 32  --num_train_epochs 50  --logging_dir $LOG_DIR/$MODEL_TYPE/$LR  --evaluation_strategy steps  --remove_unused_columns False  --overwrite_output_dir True  --report_to tensorboard  --seed {j}  --labeltxt D:\mymodel\8-5maindataset\\{i}\labels.txt  --labelnum {numm}"
            classtest1 = f"python -m OpenLP.driver.mytest_class  --output_dir D:/Patton-main/data/sports/class_task/tmp  --model_name_or_path D:\mymodel\classend  --tokenizer_name D:\mymodel\classend  --model_type graphformer  --do_eval  --train_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --eval_path D:/mymodel/8-5maindataset/{i}/{i}.pt  --fp16  --per_device_eval_batch_size 4  --max_len 32   --evaluation_strategy steps  --remove_unused_columns False  --overwrite_output_dir True  --dataloader_num_workers 0  --seed {j}  --labeltxt D:\mymodel\8-5maindataset\\{i}\labels.txt  --labelnum {numm}"
            run_commands([classcom1,classtest1], encoding="gbk")
            #run_commands([classtest1], encoding="gbk")

    for i in lmdatasets:
        for j in randomseed:
            if i in retribias:
                break
            file_path = Path("D:/mymodel/truedataset/final/retrieve/token/embeddings.query.rank.0")  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
            try:
                if file_path.exists():
                    file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                    print(f"âœ… æ–‡ä»¶ {file_path} å·²æˆåŠŸåˆ é™¤")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
            except FileNotFoundError:
                print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            file_path = Path("D:/mymodel/truedataset/final/retrieve/token/sports.embeddings.corpus.rank.0")  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
            try:
                if file_path.exists():
                    file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                    print(f"âœ… æ–‡ä»¶ {file_path} å·²æˆåŠŸåˆ é™¤")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
            except FileNotFoundError:
                print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            file_path = Path("D:/mymodel/truedataset/final/retrieve/token/sports_sports_retrieval_dict.pkl")  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
            try:
                if file_path.exists():
                    file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                    print(f"âœ… æ–‡ä»¶ {file_path} å·²æˆåŠŸåˆ é™¤")
                else:
                    print(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
            except FileNotFoundError:
                print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
            retri1=f"python -m OpenLP.driver.trainrerankeval  --output_dir D:/mymodel/retrieveend  --model_name_or_path D:/mymodel/formal/{i}/{modelname}   --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --do_train  --hn_num 4  --save_steps 80  --eval_steps 80  --logging_steps 80  --train_path D:/mymodel/8-5maindataset/{i}/retrieve/train16.jsonl  --eval_path D:/mymodel/8-5maindataset/{i}/retrieve/val.jsonl  --fp16  --per_device_train_batch_size 4   --per_device_eval_batch_size 4  --learning_rate 1e-5     --max_len 32   --num_train_epochs 100   --logging_dir D:/Patton-main/logs/sports/link_prediction     --evaluation_strategy steps  --remove_unused_columns False  --overwrite_output_dir True  --report_to tensorboard  --seed {j}"
            retri2=f"python -m OpenLP.driver.infer  --output_dir D:/mymodel/truedataset/final/retrieve/token  --model_name_or_path D:/mymodel/retrieveend  --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --per_device_eval_batch_size 4  --corpus_path D:/mymodel/8-5maindataset/{i}/retrieve/documents.txt   --doc_column_names id,text  --max_len 32  --retrieve_domain sports  --dataloader_num_workers 0"
            retri3=f"python -m OpenLP.driver.search  --output_dir D:/mymodel/truedataset/final/retrieve/token  --model_name_or_path D:/mymodel/retrieveend  --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --per_device_eval_batch_size 4  --corpus_path D:/mymodel/8-5maindataset/{i}/retrieve/documents.txt  --query_path D:/mymodel/8-5maindataset/{i}/retrieve/test.node.text.jsonl  --query_column_names id,text  --max_len 32  --save_trec True  --retrieve_domain sports  --source_domain sports  --save_path D:/mymodel/truedataset/final/retrieve/token/retrieve  --dataloader_num_workers 0"
            retri4=f"python -m trec  --truth_path D:/mymodel/8-5maindataset/{i}/retrieve/test.truth.trec"
            run_commands([retri1,retri2,retri3,retri4], encoding="gbk")
            #import error
        for j in randomseed:
            if i in rerankbia:
                break
            rerank1=f"python -m OpenLP.driver.trainrerankeval  --output_dir D:/mymodel/rerank  --model_name_or_path D:/mymodel/formal/{i}/{modelname}   --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --do_train  --hn_num 4  --save_steps 48  --eval_steps 48  --logging_steps 48  --train_path D:/mymodel/8-5maindataset/{i}/rank/train32.rerank.jsonl  --eval_path D:/mymodel/8-5maindataset/{i}/rank/val.rerank.jsonl  --fp16  --per_device_train_batch_size 4   --per_device_eval_batch_size 4  --learning_rate 1e-5     --max_len 32   --num_train_epochs 30   --logging_dir D:/Patton-main/logs/sports/link_prediction     --evaluation_strategy steps  --remove_unused_columns False  --overwrite_output_dir True  --report_to tensorboard  --seed {j}"
            rerank2=f"python -m OpenLP.driver.mytest_rerank  --output_dir $TEST_DIR/tmp  --model_name_or_path D:/mymodel/rerank  --tokenizer_name D:/mymodel/formal/{i}/{modelname}  --model_type graphformer  --do_eval  --pos_rerank_num 5  --neg_rerank_num 45  --train_path D:/mymodel/8-5maindataset/{i}/rank/test.rerank.jsonl  --eval_path D:/mymodel/8-5maindataset/{i}/rank/test.rerank.jsonl  --fp16  --per_device_eval_batch_size 4  --max_len 32  --evaluation_strategy steps  --remove_unused_columns False  --overwrite_output_dir True  --dataloader_num_workers 0 --seed {j}"
            run_commands([rerank1, rerank2], encoding="gbk")





